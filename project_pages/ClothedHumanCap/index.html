<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Clothed Human Performance Capture with a Double-layer Neural Radiance Fields</title>
    <!-- Bootstrap -->
    <link rel="stylesheet" type="text/css" href="CSS/bootstrap-4.4.1.css">
    <link rel="stylesheet" href="CSS/font-awesome.min.css">
    <link rel="stylesheet" href="CSS/index.css">
  </head>
  <div class="containrt">
    <div class="jumbotron text-center" style="margin-bottom:0">
      <h2>Clothed Human Performance Capture with a Double-layer Neural Radiance Fields</h2>
      <hr>
      <div class="row">
        <div class="offset-sm-3 col-sm-6">
          <div class="author-block">
            <span>
              <a href="https://wangkangkan.github.io/">Kangkan Wang</a><sup>*1,2</sup>,
            </span>
            <span>
              Guofeng Zhang<sup>3</sup>,
            </span>
            <span>
              Suxu Cong<sup>1</sup>,
            </span>
            <span>
              Jian Yang<sup>1,2</sup>
            </span>
            <hr>
          </div>
        </div>
      </div>
      <div class="author-block">
        <span><sup>1</sup>Key Lab of Intelligent Perception and Systems for High-Dimensional Information of Ministry of Education,</span><br>
        <span><sup>2</sup>Jiangsu Key Lab of Image and Video Understanding for Social Security,School of Computer Science and Engineering, <br>Nanjing University of Science and Technology, China</span><br>
        <span><sup>3</sup>State Key Laboratory of CAD&CG, Zhejiang University, China</span>
      </div>
      <p style="margin-top: 20px;">
        <a class="btn btn-primary" href=""><i class="fa fa-file"></i> Paper</a>
        <a class="btn btn-primary" href="https://github.com/"><i class="fa fa-github"></i> Code(Coming Soon)</a>
        <!-- <a class="btn btn-primary" href="https://ieeexplore.ieee.org/abstract/document/9870173/media#media"><i class="fa fa-video-camera"></i> Video</a> -->
      </p>
    </div>
  </div>

  <div class="container" style="margin-top:20px;">
    <div class="row">
      <div class="col-sm-4">
        <h3>Abstract</h3>
        <hr>
      </div>
    </div>
    <div class="row">
      <div class="col-12 text-center">
        <div class="pipeline-block"><img src="img/pipeline.png" class="img-responsive" style="width: 80%;"></div>
        <div class="abstract-block">
          <p class="text-left">
            This paper addresses the challenge of capturing performance for the clothed humans from sparse-view or monocular videos. Previous methods capture the performance of
            full humans with a personalized template or recover the
            garments from a single frame with static human poses.
            However, it is inconvenient to extract cloth semantics and
            capture clothing motion with one-piece template, while single frame-based methods may suffer from instable tracking
            across videos. To address these problems, we propose a
            novel method for human performance capture by tracking
            clothing and human body motion separately with a doublelayer neural radiance fields (NeRFs). Specifically, we propose a double-layer NeRFs for the body and garments, and
            track the densely deforming template of the clothing and
            body by jointly optimizing the deformation fields and the
            canonical double-layer NeRFs. In the optimization, we introduce a physics-aware cloth simulation network which
            can help generate physically plausible cloth dynamics and
            body-cloth interactions. Compared with existing methods, our method is fully differentiable and can capture both
            the body and clothing motion robustly from dynamic videos.
            Also, our method represents the clothing with an independent NeRFs, allowing us to model implicit fields of general
            clothes feasibly. The experimental evaluations validate its
            effectiveness on real multi-view or monocular videos.        
      </p>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <div class="row">
      <div class="col-sm-4">
        <h3>Result on Different Dataset</h3>
        <hr>
      </div>
      <div class="col-12 text-center">
        <img src="img/result.png" class="img-responsive" style="width: 50%;">
        <div class="description">
          <p>S4 from DeepCap Dataset(Left); "FranziRed" from DynaCap Dataset(Middle); S1 from DeepCap Dataset(Right)</p>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <div class="row">
      <div class="col-sm-4">
        <h3>Supplementary Video</h3>
        <hr>
      </div>
      <div class="col-12 text-center">
        <video controls="controls" width="80%" autoplay="autoplay"  volume="1" id="myVideo"  loop="loop">
          <source id="introduction" src="video/introduction.mp4" type="video/mp4"/>
        </video> 
      </div>
    </div>
  </div>

  <div class="container">
    <div class="row">
      <div class="col-sm-5">
        <h3>Application of Cloth Retargeting</h3>
        <hr>
      </div>
      <div class="col-12 text-center">
        <img src="img/clothes.png" class="img-responsive" style="width: 50%;">
        <div class="description">
          <p>Retargeting the clothing between two persons</p>
        </div>
      </div>
    </div>
  </div>

  <div class="container">
    <div class="row">
      <div class="col-sm-4">
        <h3>Citation</h3>
        <hr>
      </div>
    </div>
    <div class="row">
      <div class="col-12 text-left">
        <p>If you find this code useful for your research, please use the following BibTeX entry.</p>
        <pre style="background-color: #eef0f4; padding-left: 20px;" class="rounded">
          <code>
@inproceedings{Wang2022ClothedHumanCap,
  author = {Kangkan Wang*, Guofeng Zhang, Suxu Cong, Jian Yang}, 
  title = {Clothed Human Performance Capture with a Double-layer Neural Radiance Fields, 
  booktitle = {Computer Vision and Pattern Recognition (CVPR)},
  year = {2023},
  }
          </code>
      </pre>
      </div>
    </div>
  </div>
  
</html>